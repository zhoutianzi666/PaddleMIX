seed: 0
trainer:
  precision: bfloat16
  max_epochs: 1
  max_steps: 10
  num_sanity_val_steps: 5
  log_every_n_steps: 5
  save_checkpoint_steps: 
  save_checkpoint_epochs: 1
  save_path: "checkpoints"


model:
  class_path: taming.models.lfqgan.VQModel
  init_args:
    ddconfig:
      double_z: False
      z_channels: 18
      resolution: 128
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult: [1,1,2,2,4]  # num_down = len(ch_mult)-1
      num_res_blocks: 4

    lossconfig:
      target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
      params:
        disc_conditional: False
        disc_in_channels: 3
        disc_start: 0 # from 0 epoch
        disc_weight: 0.8
        gen_loss_weight: 0.1
        lecam_loss_weight: 0.05
        codebook_weight: 0.1
        commit_weight: 0.25
        codebook_enlarge_ratio: 0
        codebook_enlarge_steps: 2000

    n_embed: 262144
    embed_dim: 18
    learning_rate: 1e-4
    sample_minimization_weight: 1.0
    batch_maximization_weight: 1.0
    scheduler_type: "None"
    use_ema: True
    resume_lr:
    lr_drop_epoch: [200, 250]

data:
  class_path: main.DataModuleFromConfig
  init_args:
    batch_size: 8
    num_workers: 16
    train:
      target: taming.data.imagenet.ImageNetTrain
      params:
        config:
          size: 256
          subset:
    validation:
      target: taming.data.imagenet.ImageNetValidation
      params:
        config:
          size: 256
          subset:
    test:
      target: taming.data.imagenet.ImageNetValidation
      params:
        config:
          size: 256
          subset:

ckpt_path: null # to resume