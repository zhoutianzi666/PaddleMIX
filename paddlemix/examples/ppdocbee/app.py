# Copyright (c) 2024 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import hashlib
import os
import os.path
import sys
import tempfile
import time
from datetime import datetime

import gradio as gr
import numpy as np
import paddle
from PIL import Image

# è®¾ç½®ä½¿ç”¨çš„GPUè®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# æ¨¡å‹é…ç½®
model_path = "PaddleMIX/PPDocBee-2B-1129"
dtype = "bfloat16"  # V100è¯·æ”¹æˆfloat16

# å…¨å±€å˜é‡å®šä¹‰
model = None
processor = None

min_pixels = 256 * 28 * 28  # æœ€å°åƒç´ æ•°
max_pixels = 48 * 48 * 28 * 28  # æœ€å¤§åƒç´ æ•°

SERVER_NAME = "localhost"
SERVER_PORR = 8080


def check_and_install_paddlemix():
    try:
        from paddlemix.models.qwen2_vl.modeling_qwen2_vl import (
            Qwen2VLForConditionalGeneration,
        )

        print("Required Qwen2VL model successfully installed")
    except ImportError:
        print("Failed to install required Qwen2VL model even after running the script")
        sys.exit(1)


# åœ¨ç»§ç»­ä¹‹å‰æ£€æŸ¥æ‰€éœ€æ¨¡å‹
check_and_install_paddlemix()


from paddlemix.models.qwen2_vl import MIXQwen2Tokenizer
from paddlemix.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLForConditionalGeneration
from paddlemix.processors.qwen2_vl_processing import (
    Qwen2VLImageProcessor,
    Qwen2VLProcessor,
    process_vision_info,
)

# ç¤ºä¾‹ä½¿ç”¨HTTPé“¾æ¥
EXAMPLES = [
    [
        "ç»´ä¿®ä¿å…»ã€å…¶ä»–æ³¨æ„äº‹é¡¹çš„æ³¨æ„ç‚¹ä¸­ï¼Œç”µæ± éœ€ä¸ºä»€ä¹ˆå‹å·çš„ï¼Ÿ",
        "paddlemix/demo_images/shuomingshu_20.png",
    ],
    [
        "äº§å“æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "paddlemix/demo_images/shuomingshu_39.png",
    ],
]


class ImageCache:
    """å›¾ç‰‡ç¼“å­˜ç®¡ç†ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–å›¾ç‰‡ç¼“å­˜"""
        self.temp_dir = tempfile.mkdtemp()
        self.current_image = None
        self.is_example = False  # æ ‡è®°å½“å‰å›¾ç‰‡æ˜¯å¦ä¸ºç¤ºä¾‹å›¾ç‰‡
        print(f"Created temporary directory for image cache: {self.temp_dir}")

    def cleanup_previous(self):
        """æ¸…ç†ä¹‹å‰çš„ç¼“å­˜å›¾ç‰‡"""
        if self.current_image and os.path.exists(self.current_image) and not self.is_example:
            try:
                os.unlink(self.current_image)
                print(f"Cleaned up previous image: {self.current_image}")
            except Exception as e:
                print(f"Error cleaning up previous image: {e}")

    def cache_image(self, image_path, is_example=False):
        """
        ç¼“å­˜å›¾ç‰‡å¹¶è¿”å›ç¼“å­˜è·¯å¾„
        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            is_example: æ˜¯å¦ä¸ºç¤ºä¾‹å›¾ç‰‡
        Returns:
            ç¼“å­˜åçš„å›¾ç‰‡è·¯å¾„
        """
        if not image_path:
            return None

        try:
            # å¦‚æœæ˜¯ç¤ºä¾‹å›¾ç‰‡ä¸”å·²ç»åœ¨ä½¿ç”¨ä¸­ï¼Œç›´æ¥è¿”å›
            if is_example and self.current_image == image_path and self.is_example:
                return self.current_image

            # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_hash = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]
            _, ext = os.path.splitext(image_path)
            if not ext:
                ext = ".jpg"  # é»˜è®¤æ‰©å±•å
            new_filename = f"image_{timestamp}_{file_hash}{ext}"

            # åœ¨ä¸´æ—¶ç›®å½•ä¸­åˆ›å»ºæ–°è·¯å¾„
            new_path = os.path.join(self.temp_dir, new_filename) if not is_example else image_path

            if not is_example:
                # å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶
                with Image.open(image_path) as img:
                    # å¦‚æœéœ€è¦ï¼Œè½¬æ¢ä¸ºRGB
                    if img.mode != "RGB":
                        img = img.convert("RGB")
                    img.save(new_path)

                # æ›´æ–°å½“å‰å›¾ç‰‡ä¹‹å‰æ¸…ç†ä¹‹å‰çš„å›¾ç‰‡
                self.cleanup_previous()

            self.current_image = new_path
            self.is_example = is_example

            return new_path

        except Exception as e:
            print(f"Error caching image: {e}")
            return image_path


# åˆ›å»ºå…¨å±€å›¾ç‰‡ç¼“å­˜ç®¡ç†å™¨
image_cache = ImageCache()


def load_model():
    """åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œå†…å­˜ä¼˜åŒ–"""
    global model, processor

    if model is None:
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            dtype=dtype,
        )
        image_processor = Qwen2VLImageProcessor()
        tokenizer = MIXQwen2Tokenizer.from_pretrained(model_path)
        processor = Qwen2VLProcessor(image_processor, tokenizer, min_pixels=min_pixels, max_pixels=max_pixels)

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
    del tokenizer
    return model, processor


def clear_cache():
    """æ¸…ç†GPUç¼“å­˜"""
    if paddle.device.cuda.memory_allocated() > 0:
        paddle.device.cuda.empty_cache()
        import gc

        gc.collect()


def multimodal_understanding(image, question, seed=42, top_p=0.95, temperature=0.1):
    """
    å¤šæ¨¡æ€ç†è§£ä¸»å‡½æ•°
    Args:
        image: è¾“å…¥å›¾ç‰‡
        question: é—®é¢˜æ–‡æœ¬
        seed: éšæœºç§å­
        top_p: é‡‡æ ·å‚æ•°
        temperature: æ¸©åº¦å‚æ•°
    Yields:
        å¤„ç†çŠ¶æ€å’Œç»“æœ
    """
    # è¾“å…¥éªŒè¯
    if not image:
        yield "âš ï¸ è¯·ä¸Šä¼ å›¾ç‰‡åå†å¼€å§‹å¯¹è¯ã€‚"
        return
    if not question or question.strip() == "":
        yield "âš ï¸ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜åå†å¼€å§‹å¯¹è¯ã€‚"
        return

    try:
        start_time = time.time()
        yield "ğŸ”„ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œè¯·ç¨å€™..."

        # æ£€æŸ¥è¶…æ—¶
        if time.time() - start_time > 200:
            yield "â³ ç³»ç»Ÿå½“å‰ç”¨æˆ·ç¹å¤šï¼Œè¯·ç­‰å¾…10åˆ†é’Ÿåå†æ¬¡å°è¯•ã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£ï¼"
            return

        clear_cache()

        # è®¾ç½®éšæœºç§å­
        paddle.seed(seed)
        np.random.seed(seed)

        # å¤„ç†å›¾ç‰‡ç¼“å­˜
        is_example = any(image == example[1] for example in EXAMPLES)
        cached_image = image_cache.cache_image(image, is_example=is_example)
        if not cached_image:
            return "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚"

        # æ„å»ºæç¤ºæ–‡æœ¬
        prompts = question + "\nè¯·ç”¨å›¾ç‰‡ä¸­å®Œæ•´å‡ºç°çš„å†…å®¹å›ç­”ï¼Œå¯ä»¥æ˜¯å•è¯ã€çŸ­è¯­æˆ–å¥å­ï¼Œé’ˆå¯¹é—®é¢˜å›ç­”å°½å¯èƒ½è¯¦ç»†å’Œå®Œæ•´ï¼Œå¹¶ä¿æŒæ ¼å¼ã€å•ä½ã€ç¬¦å·å’Œæ ‡ç‚¹éƒ½ä¸å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹å®Œå…¨ä¸€è‡´ã€‚"

        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": cached_image,
                    },
                    {"type": "text", "text": prompts},
                ],
            }
        ]

        yield "æ¨¡å‹æ­£åœ¨åˆ†æå›¾ç‰‡å†…å®¹..."

        # å¤„ç†è§†è§‰ä¿¡æ¯
        image_inputs, video_inputs = process_vision_info(messages)
        image_pad_token = "<|vision_start|><|image_pad|><|vision_end|>"
        text = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„å¤šæ¨¡æ€ç†è§£çš„AIåŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n{image_pad_token}{prompts}<|im_end|>\n<|im_start|>assistant\n"

        # ç”Ÿæˆå›ç­”
        with paddle.no_grad():
            inputs = processor(
                text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pd"
            )

            yield "æ­£åœ¨ç”Ÿæˆå›ç­”..."

            generated_ids = model.generate(
                **inputs,
                max_new_tokens=1024,
                top_p=top_p,
                temperature=temperature,
                num_beams=1,
                do_sample=True,
                use_cache=True,
            )

            output_text = processor.batch_decode(
                generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]

        # æ¸…ç†å†…å­˜
        del inputs, generated_ids
        clear_cache()

        yield output_text

    except Exception as e:
        error_message = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\nè¯·é‡è¯•æˆ–åœ¨è¯„è®ºåŒºç•™ä¸‹ä½ çš„é—®é¢˜ã€‚"
        return error_message


def process_example(question, image):
    """å¤„ç†ç¤ºä¾‹å›¾ç‰‡çš„åŒ…è£…å‡½æ•°"""
    cached_path = image_cache.cache_image(image, is_example=True)
    return multimodal_understanding(cached_path, question)


def handle_image_upload(image):
    """å¤„ç†å›¾ç‰‡ä¸Šä¼ """
    if image is None:
        return None
    try:
        cached_path = image_cache.cache_image(image, is_example=False)
        return cached_path
    except Exception as e:
        print(f"Error handling image upload: {e}")
        return None


# model, processor = load_model()
# # image = "/home/aistudio/work/doc-lark/PaddleMIX/paddlemix/demo_images/examples_image1.jpg"
# print(multimodal_understanding(EXAMPLES[1][1],EXAMPLES[1][0]))

# Gradioç•Œé¢é…ç½®
with gr.Blocks() as demo:
    gr.Markdown(
        value="""
    # ğŸ¤– PP-DocBee(2B): Multimodal Document Understanding Demo

    ğŸ“š åŸå§‹æ¨¡å‹æ¥è‡ª [PaddleMIX](https://github.com/PaddlePaddle/PaddleMIX)  ï¼ˆğŸŒŸ ä¸€ä¸ªåŸºäºé£æ¡¨PaddlePaddleæ¡†æ¶æ„å»ºçš„å¤šæ¨¡æ€å¤§æ¨¡å‹å¥—ä»¶ï¼‰
    """
    )
    with gr.Row():
        image_input = gr.Image(type="filepath", label="ğŸ“· Upload Image or Input URL")
        with gr.Column():
            question_input = gr.Textbox(label="ğŸ’­ Question", placeholder="Enter your question here...")
            und_seed_input = gr.Number(label="ğŸ² Seed", precision=0, value=42)
            top_p = gr.Slider(minimum=0, maximum=1, value=0.95, step=0.05, label="ğŸ“Š Top P")
            temperature = gr.Slider(minimum=0, maximum=1, value=0.1, step=0.05, label="ğŸŒ¡ï¸ Temperature")

    image_input.upload(fn=handle_image_upload, inputs=[image_input], outputs=[image_input])

    understanding_button = gr.Button("ğŸ’¬ Chat", variant="primary")
    understanding_output = gr.Textbox(label="ğŸ¤– Response", interactive=False)

    gr.Examples(
        examples=EXAMPLES,
        inputs=[question_input, image_input],
        outputs=understanding_output,
        fn=process_example,
        cache_examples=True,
        run_on_click=True,
    )

    # åŠ è½½æ¨¡å‹
    clear_cache()
    model, processor = load_model()
    clear_cache()

    understanding_button.click(
        fn=multimodal_understanding,
        inputs=[image_input, question_input, und_seed_input, top_p, temperature],
        outputs=understanding_output,
        api_name="chat",
    )

if __name__ == "__main__":
    # åˆ›å»ºé˜Ÿåˆ—
    demo.queue()
    demo.launch(server_name=SERVER_NAME, server_port=SERVER_PORR, share=True, ssr_mode=False, max_threads=1)  # é™åˆ¶å¹¶å‘è¯·æ±‚æ•°
